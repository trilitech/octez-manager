# Integration Test Sharding

This directory uses time-based sharding to balance CI test execution across parallel jobs.

## Overview

Instead of splitting tests by category (node, baker, dal, accuser, import), we distribute tests across **shards** based on their execution time. This ensures all CI jobs finish around the same time, minimizing total wall-clock time.

## Files

- **`shards.json`** - Manifest mapping tests to shards
  - Checked into git, versioned
  - Generated by `scripts/generate-shard-manifest.py`
  - Updated manually when test balance degrades

- **`run-tests.sh`** - Test runner with shard support
  - Legacy mode: `./run-tests.sh node` (run by category)
  - Shard mode: `./run-tests.sh --shard 1` (run shard 1)
  - All mode: `./run-tests.sh all` (run all tests)

## Shard Manifest Structure

```json
{
  "metadata": {
    "generated_at": "2026-01-21T12:00:00",
    "total_tests": 62,
    "total_duration": 913.5,
    "shard_count": 4
  },
  "shard-1": {
    "tests": [
      "node/09-snapshot-import.sh",
      "baker/41-baker-with-dal-node.sh",
      ...
    ],
    "test_count": 16,
    "total_duration": 231.0
  },
  ...
}
```

## How It Works

### 1. Test Collection
Tests in all categories are timed during execution:
```bash
# In run-tests.sh
start=$(date +%s.%N)
bash test.sh
end=$(date +%s.%N)
echo "TIMING: /tests/node/01-install.sh $(echo $end - $start | bc)" >> /tmp/test-timings.log
```

### 2. Shard Generation
The Python script uses **greedy bin-packing** (Longest Processing Time algorithm):

```python
# 1. Sort tests by duration (longest first)
sorted_tests = sorted(tests.items(), key=lambda x: x[1], reverse=True)

# 2. Assign each test to least-loaded shard
for test, duration in sorted_tests:
    min_shard = min(shards, key=lambda s: s["total_duration"])
    min_shard["tests"].append(test)
    min_shard["total_duration"] += duration
```

This produces near-optimal balance in O(n log n) time.

### 3. CI Execution
GitHub Actions runs 4 jobs in parallel:

```yaml
strategy:
  matrix:
    shard: [1, 2, 3, 4]

steps:
  - run: docker compose exec -T cli-tester /run-tests.sh --shard ${{ matrix.shard }}
```

Each job runs a subset of tests with approximately equal total duration.

## Updating Shards

### When to Update

Update `shards.json` when:
- Many tests added/removed
- Test durations changed significantly
- Shard imbalance >20% (one shard takes much longer than others)
- Quarterly maintenance

### How to Update

#### Option 1: From CI Logs (Recommended)

```bash
# 1. Download latest CI run logs
gh run view <run-id> --log > ci-logs.txt

# 2. Extract timings
grep "TIMING:" ci-logs.txt > test-timings.txt

# 3. Generate new manifest
python3 scripts/generate-shard-manifest.py test-timings.txt > test/integration/cli-tester/tests/shards.json

# 4. Review balance
cat test/integration/cli-tester/tests/shards.json | jq '.metadata'

# 5. Commit
git add test/integration/cli-tester/tests/shards.json
git commit -m "chore(ci): update shard manifest"
```

#### Option 2: From Local Test Run

```bash
# 1. Run all tests locally
cd test/integration
docker compose up -d --build
docker compose exec -T cli-tester /run-tests.sh all 2>&1 | tee test-run.log

# 2. Extract timings from container
docker compose exec -T cli-tester cat /tmp/test-timings.log > test-timings.txt

# 3. Generate manifest
python3 ../../scripts/generate-shard-manifest.py test-timings.txt \
  > cli-tester/tests/shards.json

# 4. Verify and commit
git add cli-tester/tests/shards.json
git commit -m "chore(ci): update shard manifest"
```

## Manifest Generator Options

```bash
# Basic usage
python3 scripts/generate-shard-manifest.py timings.txt > shards.json

# Custom shard count
python3 scripts/generate-shard-manifest.py timings.txt --shards 6 > shards.json

# Quiet mode (no stderr summary)
python3 scripts/generate-shard-manifest.py timings.txt --quiet > shards.json
```

The script outputs:
- **stdout**: JSON manifest
- **stderr**: Human-readable summary with balance statistics

## Balance Metrics

The manifest includes a **balance coefficient** (coefficient of variation):

```
CV = (standard_deviation / mean) * 100
```

- **<10%** - Excellent balance
- **10-20%** - Good balance
- **>20%** - Consider rebalancing

Example:
```
Shard 1: 231.0s
Shard 2: 232.0s
Shard 3: 225.0s
Shard 4: 225.0s

Mean: 228.25s
Std dev: 3.77s
CV: 1.65% ← Excellent!
```

## Local Development

During development, you can still run tests by category:

```bash
# Run all node tests
./run-tests.sh node

# Run all tests
./run-tests.sh all

# Run specific shard (for debugging CI issues)
./run-tests.sh --shard 2
```

## Benefits

### Before Sharding (Category-based)
```
Job: node     [████████████████████████] 8 min
Job: baker    [████████████████        ] 6 min
Job: accuser  [████████                ] 4 min
Job: dal      [██████                  ] 3 min
Job: import   [████                    ] 2 min

Total CI time: 8 min (limited by slowest)
Parallelism efficiency: ~60%
```

### After Sharding (Time-based, 4 shards)
```
Job: shard-1  [████████████████        ] 4 min
Job: shard-2  [████████████████        ] 4 min
Job: shard-3  [███████████████         ] 3.8 min
Job: shard-4  [███████████████         ] 3.8 min

Total CI time: ~4 min
Parallelism efficiency: ~95%
Speedup: 50% faster!
```

## Troubleshooting

### "Shard manifest not found"
- Check that `shards.json` exists in `test/integration/cli-tester/tests/`
- Ensure it's committed to git (not in .gitignore)

### "No tests found for shard N"
- Verify shard ID is valid (1-4)
- Check manifest has `shard-N` key with non-empty tests array

### Shard timing is way off
- Timing estimates may be inaccurate
- Run full test suite to collect real timings
- Regenerate manifest from actual data

### One shard much slower than others
- Normal variance: 10-20% is acceptable
- Large variance: >30% indicates need for rebalancing
- Update manifest from latest timings

## Future Enhancements

Potential improvements:
- Automated scheduled updates (weekly PR with new manifest)
- Dynamic shard count based on test count
- Per-test historical timing database
- Smart test ordering (flaky tests first for fast failure)

## Regenerating the Manifest with Real Timing Data

The manifest should be regenerated periodically to maintain optimal balance as tests change.

### When to Regenerate

- Test suite changes significantly (tests added/removed)
- Test durations change by >20%
- Shard balance degrades (>10% coefficient)
- After major refactoring

### Step-by-Step Process

#### 1. Get CI Run ID

Find a recent successful CI run on main:
```bash
gh run list --branch main --workflow CI --limit 5
```

#### 2. Download CI Logs

```bash
gh run view <run-id> --log > ci-logs.txt
```

#### 3. Extract Timing Data

The CI workflow outputs timing data from each shard:
```bash
# Extract TIMING lines from all shards
grep '^TIMING:' ci-logs.txt > test-timings.txt

# Verify count (should be 60-61 tests)
echo "Tests found: $(wc -l < test-timings.txt)"
```

Example timing data:
```
TIMING: /tests/node/17-preserve-data-resume.sh 18.643577024
TIMING: /tests/accuser/37-accuser-extra-args.sh 18.531480509
TIMING: /tests/baker/32-baker-node-dependency.sh 12.979262923
```

#### 4. Generate New Manifest

```bash
# Generate with 5 shards (or adjust as needed)
python3 scripts/generate-shard-manifest.py test-timings.txt --shards 5 \
  > test/integration/cli-tester/tests/shards.json
```

Review the balance output:
```
=== Shard Manifest Summary ===
Total tests: 60
Total duration: 392.6s
Shards: 5

Shard balance:
  Mean: 78.5s
  Range: 77.5s - 78.8s
  Std dev: 0.5s
  Balance coefficient: 0.6% (lower is better)

✓ Excellent balance (variance <10%)
```

#### 5. Verify and Commit

```bash
# Verify all test paths exist
cd test/integration/cli-tester/tests
for test in $(jq -r '.[] | select(.test_count) | .tests[]' shards.json 2>/dev/null); do
  [ -f "$test" ] || echo "MISSING: $test"
done

# Commit
git add shards.json
git commit -m "chore(ci): update shard manifest with timings from CI run <run-id>

Balance coefficient: X.X%
Mean shard duration: XXs"
```

### Troubleshooting

**No TIMING lines in logs:**
- Check that `bc` is installed in the test container (Dockerfile)
- Verify the "Output timing data" CI step ran
- Ensure tests completed successfully

**Wrong test count:**
- Some tests may have failed in the CI run
- Use a fully successful run for timing data
- Check if new tests were added since last update

**Test paths don't exist:**
- Manual path corrections may be needed
- Test names containing "dal" or "baker" may be misclassified
- Verify paths match actual file locations

### Timing Data Quality

With `bc` installed, timings have nanosecond precision:
- ✅ `18.643577024s` - precise
- ❌ `0s` - means `bc` is missing

The test runner logs timing to `/tmp/test-timings.log` and the CI workflow outputs it in the "Output timing data" step.
